{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM8ruem3f1jDJan2ZcvKNR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chiamakac/Scraping-Tables/blob/main/CARE_EXTRACTION_02_01_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PeDOoXgc2T8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a1805c-c3aa-4a06-9808-62b93384b2ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount the goolge drive or upload the pdf files to extract\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuuL6EIpYlB1",
        "outputId": "f9a7e021-6916-47fb-c60c-a85df074f831"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q fuzzywuzzy"
      ],
      "metadata": {
        "id": "4QHAEx4kYpAd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q python-Levenshtein"
      ],
      "metadata": {
        "id": "CAWwGWH63IEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3759bff7-293c-4f48-ee9e-a638032ace22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pdfminer.layout import LTTextContainer, LTChar\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.converter import PDFPageAggregator\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.high_level import extract_text"
      ],
      "metadata": {
        "id": "x0yRzVk53UAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_pages\n",
        "from pdfminer.layout import LTTextContainer, LTChar, LAParams, PDFResourceManager, PDFPageAggregator, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "import os\n",
        "\n",
        "def extract_pages(pdf_path):\n",
        "    # PDFMiner setup for extracting pages\n",
        "    resource_manager = PDFResourceManager()\n",
        "    device = PDFPageAggregator(resource_manager, laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(resource_manager, device)\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        for page in PDFPage.get_pages(file, check_extractable=False):\n",
        "            interpreter.process_page(page)\n",
        "            layout = device.get_result()\n",
        "            yield layout\n",
        "\n",
        "def process_pdf_folder(input_folder, output_folder):\n",
        "    # Iterate over files in the specified folder\n",
        "    for root, _, files in os.walk(input_folder):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.pdf'):\n",
        "                # Build paths\n",
        "                input_file_path = os.path.join(root, filename)\n",
        "                output_file_folder = os.path.join(output_folder, os.path.relpath(input_file_path, input_folder))\n",
        "                os.makedirs(output_file_folder, exist_ok=True)\n",
        "\n",
        "                for page_number, page_layout in enumerate(extract_pages(input_file_path), start=1):\n",
        "                    output_file = os.path.join(output_file_folder, f'page{page_number}.txt')\n",
        "\n",
        "                    with open(output_file, 'w') as file:\n",
        "                        # Variables to track previous text and font size\n",
        "                        previous_text = ''\n",
        "                        previous_font_size = 0\n",
        "                        merged_text = ''\n",
        "\n",
        "                        for element in page_layout:\n",
        "                            if isinstance(element, LTTextContainer):\n",
        "                                for text_line in element:\n",
        "                                    if isinstance(text_line, LTTextContainer):\n",
        "                                        if len(text_line._objs) > 0:\n",
        "                                            first_char = text_line._objs[0]\n",
        "                                            if isinstance(first_char, LTChar):\n",
        "                                                font_size = round(first_char.size)\n",
        "                                                font_name = first_char.fontname\n",
        "                                                font_color = first_char.graphicstate.ncolor\n",
        "                                                text = element.get_text()\n",
        "\n",
        "                                                # Check if the current text is different from the previous text\n",
        "                                                if text != previous_text:\n",
        "                                                    if font_size == previous_font_size:\n",
        "                                                        # Merge the text with the previous one\n",
        "                                                        merged_text += ' ' + text\n",
        "                                                    else:\n",
        "                                                        # Write the merged text as a single entry\n",
        "                                                        if merged_text:\n",
        "                                                            file.write(f\"Font size: {previous_font_size}\\n\")\n",
        "                                                            # file.write(f\"Font color: {font_color}\\n\")\n",
        "                                                            # file.write(f\"Font name: {font_name}\\n\")\n",
        "                                                            file.write(f\"{merged_text}\\n\")\n",
        "                                                            merged_text = ''\n",
        "\n",
        "                                                        # Write the current text as a separate entry\n",
        "                                                        file.write(f\"Font size: {font_size}\\n\")\n",
        "                                                        # file.write(f\"Font color: {font_color}\\n\")\n",
        "                                                        # file.write(f\"Font name: {font_name}\\n\")\n",
        "                                                        file.write(f\"{text}\\n\")\n",
        "\n",
        "                                                    # Update previous text and font size\n",
        "                                                    previous_text = text\n",
        "                                                    previous_font_size = font_size\n",
        "\n",
        "                        # Write the final merged text, if any\n",
        "                        if merged_text:\n",
        "                            file.write(f\"Font size: {previous_font_size}\\n\")\n",
        "                            # file.write(f\"Font color: {font_color}\\n\")\n",
        "                            # file.write(f\"Font name: {font_name}\\n\")\n",
        "                            file.write(f\"{merged_text}\\n\")\n",
        "\n",
        "# Specify the path to the main folder containing subfolders and PDF files\n",
        "main_folder = '/content/drive/foldername'\n",
        "output_folder = 'output_folder_name'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Start processing the main folder and its subfolders\n",
        "process_pdf_folder(main_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "UZYfzEWx3VHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GETTING THE FONT SIZES HIGHER THAN THE FREQUENTLY USED\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Iterable, Any\n",
        "from pdfminer.high_level import extract_pages\n",
        "import os\n",
        "\n",
        "def count_font_sizes(o: Any, size_dict: dict):\n",
        "    # Recursively traverse the object to count font sizes\n",
        "    if isinstance(o, Iterable):\n",
        "        for i in o:\n",
        "            count_font_sizes(i, size_dict)\n",
        "    elif hasattr(o, 'size'):\n",
        "        # Round the font size and update the dictionary\n",
        "        size = round(o.size)\n",
        "        if size in size_dict:\n",
        "            size_dict[size] += 1\n",
        "        else:\n",
        "            size_dict[size] = 1\n",
        "\n",
        "def store_font_sizes_greater_than_most_frequent(size_dict):\n",
        "    # Find the most frequent font size\n",
        "    most_frequent_size = max(size_dict, key=size_dict.get)\n",
        "    # Filter sizes greater than the most frequent one\n",
        "    filtered_sizes = {size: count for size, count in size_dict.items() if size > most_frequent_size}\n",
        "    return filtered_sizes\n",
        "\n",
        "def save_font_sizes_to_file(filtered_sizes, output_file):\n",
        "    # Save font sizes to a text file\n",
        "    with open(output_file, 'w') as file:\n",
        "        for size, count in filtered_sizes.items():\n",
        "            file.write(f\"Font size: {size} \\n\")\n",
        "\n",
        "def process_pdf_folder(input_folder, output_folder):\n",
        "    # Iterate over PDF files in the input folder\n",
        "    for pdf_path in Path(input_folder).rglob('*.pdf'):\n",
        "        # Extract pages from the PDF\n",
        "        pages = extract_pages(pdf_path)\n",
        "\n",
        "        size_dict = {}  # Create an empty dictionary to store font size frequencies\n",
        "        count_font_sizes(pages, size_dict)\n",
        "\n",
        "        filtered_sizes = store_font_sizes_greater_than_most_frequent(size_dict)\n",
        "\n",
        "        # Create output folder if it doesn't exist\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        # Create output file path based on the PDF file name\n",
        "        output_file = os.path.join(output_folder, f\"{pdf_path.stem}_font_sizes.txt\")\n",
        "\n",
        "        # Save font sizes to the output file\n",
        "        save_font_sizes_to_file(filtered_sizes, output_file)\n",
        "\n",
        "# Specify the path to the input folder containing PDF files\n",
        "input_folder = 'foldername'\n",
        "\n",
        "# Specify the path to the output folder\n",
        "output_folder = 'font_sizes_output'\n",
        "\n",
        "# Start processing the input folder\n",
        "process_pdf_folder(input_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "duMATXcq4G5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing with the fontsizes above the frequently used and removing the  paragraphs\n",
        "#with font sizes equal or lower than the frequently used. Then save the filterd file as output\n",
        "\n",
        "import os\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Define the source folder path where your files are located.\n",
        "source_folder_path = 'file.pdf'\n",
        "\n",
        "# Define the destination folder path where you want to write the filtered files.\n",
        "destination_folder_path = 'des_output'  # Create a new folder for filtered files\n",
        "\n",
        "# Define the path to the file containing target font sizes.\n",
        "target_phrases_file = 'target_font_sizes.txt'\n",
        "\n",
        "# Define the similarity threshold (0 to 100) for fuzzy matching.\n",
        "# Increase this value to make the comparison stricter.\n",
        "similarity_threshold = 95\n",
        "\n",
        "# Create the destination folder if it doesn't exist.\n",
        "if not os.path.exists(destination_folder_path):\n",
        "    os.makedirs(destination_folder_path)\n",
        "\n",
        "# Function to check if a paragraph contains any of the target phrases using fuzzy matching.\n",
        "def paragraph_contains_target_phrases(paragraph, target_phrases):\n",
        "    for target_phrase in target_phrases:\n",
        "        similarity_ratio = fuzz.partial_ratio(paragraph, target_phrase)\n",
        "        if similarity_ratio >= similarity_threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Read target phrases from the file.\n",
        "with open(target_phrases_file, 'r') as phrases_file:\n",
        "    target_phrases = [line.strip() for line in phrases_file]\n",
        "\n",
        "# Iterate through files in the source folder.\n",
        "for root, _, files in os.walk(source_folder_path):\n",
        "    for filename in files:\n",
        "        source_file_path = os.path.join(root, filename)\n",
        "        destination_file_path = os.path.join(destination_folder_path, os.path.relpath(source_file_path, source_folder_path))\n",
        "\n",
        "        with open(source_file_path, 'r') as source_file, open(destination_file_path, 'w') as destination_file:\n",
        "            paragraph = \"\"\n",
        "            for line in source_file:\n",
        "                # Check if the line is empty, denoting the end of a paragraph.\n",
        "                if not line.strip():\n",
        "                    # Check if the paragraph contains any of the target phrases.\n",
        "                    if paragraph_contains_target_phrases(paragraph, target_phrases):\n",
        "                        destination_file.write(paragraph + '\\n')\n",
        "                    paragraph = \"\"\n",
        "                else:\n",
        "                    paragraph += line\n",
        "            # Check the last paragraph in the file.\n",
        "            if paragraph_contains_target_phrases(paragraph, target_phrases):\n",
        "                destination_file.write(paragraph + '\\n')"
      ],
      "metadata": {
        "id": "ZL9dh7RJ4vWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #write the first line(contains the font size) of any paragraph where the target phrases(Gold TOC) are found\n",
        "\n",
        "\n",
        "import os\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Define the source folder path where your files are located.\n",
        "source_folder_path = '/content/drive/MyDrive/CARE_MAIN/TopEng3/FILTERED_TopEng3'\n",
        "\n",
        "# Define the destination folder path where you want to write the filtered files.\n",
        "output_folder_path = 'output_filename'  # Create a new folder for filtered files\n",
        "\n",
        "# Define the path to the file containing target phrases.\n",
        "target_phrases_file = 'gold_section_header_file'\n",
        "\n",
        "# Define the similarity threshold (0 to 100) for fuzzy matching.\n",
        "# Increase this value to make the comparison stricter.\n",
        "similarity_threshold = 95\n",
        "\n",
        "# Create the destination folder if it doesn't exist.\n",
        "if not os.path.exists(output_folder_path):\n",
        "    os.makedirs(output_folder_path)\n",
        "\n",
        "# Function to check if a paragraph contains any of the target phrases using fuzzy matching.\n",
        "def paragraph_contains_target_phrases(paragraph, target_phrases):\n",
        "    for target_phrase in target_phrases:\n",
        "        similarity_ratio = fuzz.partial_ratio(paragraph, target_phrase)\n",
        "        if similarity_ratio >= similarity_threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Read target phrases from the file.\n",
        "with open(target_phrases_file, 'r') as phrases_file:\n",
        "    target_phrases = [line.strip() for line in phrases_file]\n",
        "\n",
        "# Iterate through files in the source folder.\n",
        "for root, _, files in os.walk(source_folder_path):\n",
        "    for filename in files:\n",
        "        source_file_path = os.path.join(root, filename)\n",
        "        relative_path = os.path.relpath(source_file_path, source_folder_path)\n",
        "        destination_file_path = os.path.join(output_folder_path, relative_path)\n",
        "\n",
        "        # Create subdirectories in the output folder if they don't exist\n",
        "        os.makedirs(os.path.dirname(destination_file_path), exist_ok=True)\n",
        "\n",
        "        with open(source_file_path, 'r') as source_file, open(destination_file_path, 'w') as destination_file:\n",
        "            paragraph = []\n",
        "            for line in source_file:\n",
        "                # Check if the line is empty, denoting the end of a paragraph.\n",
        "                if not line.strip():\n",
        "                    # Check if the paragraph contains any of the target phrases.\n",
        "                    if paragraph_contains_target_phrases(' '.join(paragraph), target_phrases):\n",
        "                        # Write the first line of the paragraph.\n",
        "                        destination_file.write(paragraph[0] + '\\n')\n",
        "                    paragraph = []\n",
        "                else:\n",
        "                    paragraph.append(line)"
      ],
      "metadata": {
        "id": "EXAVxOMYAtyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merging of the pages/files 'first line(contains the font size) of any paragraph where the target phrases(Gold section header) were found\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Folder containing text files in subfolders to sort and merge\n",
        "main_folder_path = 'filename'\n",
        "\n",
        "# Output folder where the merged content of each subfolder will be saved\n",
        "output_folder = 'merged_output_file'\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through subfolders in the main folder\n",
        "for subfolder_name in os.listdir(main_folder_path):\n",
        "    subfolder_path = os.path.join(main_folder_path, subfolder_name)\n",
        "\n",
        "    # Check if it's a subfolder\n",
        "    if os.path.isdir(subfolder_path):\n",
        "        # Output file where the merged content will be saved for each subfolder\n",
        "        output_file = os.path.join(output_folder, f'{subfolder_name}_merged.txt')\n",
        "\n",
        "        # Create a set to store unique sentences\n",
        "        unique_sentences = set()\n",
        "\n",
        "        # Get a list of text files in the subfolder\n",
        "        text_files = [f for f in os.listdir(subfolder_path) if f.endswith('.txt')]\n",
        "\n",
        "        # Sort the list of file names\n",
        "        sorted_files = sorted(text_files)\n",
        "\n",
        "        # Iterate through the sorted files and extract unique sentences\n",
        "        for filename in sorted_files:\n",
        "            file_path = os.path.join(subfolder_path, filename)\n",
        "\n",
        "            # Open and read each text file\n",
        "            with open(file_path, 'r') as input_file:\n",
        "                content = input_file.read()\n",
        "\n",
        "                # Split the content into sentences (assuming sentences are separated by periods)\n",
        "                sentences = content.split('. ')\n",
        "\n",
        "                # Add unique sentences to the set\n",
        "                unique_sentences.update(sentences)\n",
        "\n",
        "        # Open the output file for writing\n",
        "        with open(output_file, 'w') as merged_file:\n",
        "            # Write the unique sentences to the merged file\n",
        "            for sentence in unique_sentences:\n",
        "                merged_file.write(sentence)\n"
      ],
      "metadata": {
        "id": "8z2iNzeDFpcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the output from the previous cell with the filtered file. Check for paragraphs that contains any of the target phrases(merged_section header) and start writing from the second line\n",
        "\n",
        "\n",
        "# import os\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Define the source folder path where your files are located.\n",
        "source_folder_path = 'FILTERED_file'\n",
        "\n",
        "# Define the destination folder path where you want to write the filtered files.\n",
        "destination_folder_path = 'ouput_file'  # Create a new folder for filtered files\n",
        "\n",
        "# Define the path to the file containing target phrases.\n",
        "target_phrases_file = 'merged_fontsizes file.txt'\n",
        "\n",
        "# Define the similarity threshold (0 to 100) for fuzzy matching.\n",
        "# Increase this value to make the comparison stricter.\n",
        "similarity_threshold = 95\n",
        "\n",
        "# Create the destination folder if it doesn't exist.\n",
        "if not os.path.exists(destination_folder_path):\n",
        "    os.makedirs(destination_folder_path)\n",
        "\n",
        "# Function to check if a paragraph contains any of the target phrases using fuzzy matching.\n",
        "def paragraph_contains_target_phrases(paragraph, target_phrases):\n",
        "    for target_phrase in target_phrases:\n",
        "        similarity_ratio = fuzz.partial_ratio(paragraph, target_phrase)\n",
        "        if similarity_ratio >= similarity_threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Read target phrases from the file.\n",
        "with open(target_phrases_file, 'r') as phrases_file:\n",
        "    target_phrases = [line.strip() for line in phrases_file]\n",
        "\n",
        "# Iterate through files in the source folder.\n",
        "for root, _, files in os.walk(source_folder_path):\n",
        "    for filename in files:\n",
        "        source_file_path = os.path.join(root, filename)\n",
        "        destination_file_path = os.path.join(destination_folder_path, os.path.relpath(source_file_path, source_folder_path))\n",
        "\n",
        "        with open(source_file_path, 'r') as source_file, open(destination_file_path, 'w') as destination_file:\n",
        "            paragraph = \"\"\n",
        "            write_paragraph = False  # Flag to control writing from the second line onwards\n",
        "            for line in source_file:\n",
        "                # Check if the line is empty, denoting the end of a paragraph.\n",
        "                if not line.strip():\n",
        "                    # Check if the paragraph contains any of the target phrases.\n",
        "                    if paragraph_contains_target_phrases(paragraph, target_phrases):\n",
        "                        # Write the paragraph starting from the second line if the flag is set\n",
        "                        lines = paragraph.split('\\n')\n",
        "                        if write_paragraph:\n",
        "                            destination_file.write('\\n'.join(lines[1:]) + '\\n')\n",
        "                        write_paragraph = False\n",
        "\n",
        "                    paragraph = \"\"\n",
        "                else:\n",
        "                    paragraph += line\n",
        "                    if paragraph_contains_target_phrases(paragraph, target_phrases):\n",
        "                        write_paragraph = True\n",
        "\n",
        "            # Check the last paragraph in the file.\n",
        "            if paragraph_contains_target_phrases(paragraph, target_phrases):\n",
        "                # Write the last paragraph starting from the second line if the flag is set\n",
        "                lines = paragraph.split('\\n')\n",
        "                if write_paragraph:\n",
        "                    destination_file.write('\\n'.join(lines[1:]) + '\\n')\n"
      ],
      "metadata": {
        "id": "IhXKjLJCF7r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Write the paragraph to the output file if it has less than 9 words\n",
        "\n",
        "import os\n",
        "\n",
        "def process_folder(input_folder, output_folder):\n",
        "    for item in os.listdir(input_folder):\n",
        "        item_path = os.path.join(input_folder, item)\n",
        "\n",
        "        if os.path.isdir(item_path):\n",
        "            # If it's a subfolder, recursively process it\n",
        "            subfolder_name = os.path.basename(item_path)\n",
        "            subfolder_output = os.path.join(output_folder, subfolder_name)\n",
        "            os.makedirs(subfolder_output, exist_ok=True)\n",
        "            process_folder(item_path, subfolder_output)\n",
        "        elif item.endswith('.txt'):\n",
        "            # If it's a file, process it and save the output\n",
        "            input_file_path = item_path\n",
        "            output_file_path = os.path.join(output_folder, item)\n",
        "\n",
        "            with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
        "                inside_paragraph = False\n",
        "                paragraph_lines = []\n",
        "\n",
        "                for line in input_file:\n",
        "                    # Detect the end of a paragraph (an empty line)\n",
        "                    if not line.strip():\n",
        "                        # Check if the paragraph has less than 9 words\n",
        "                        if len(paragraph_lines) < 9:\n",
        "                            output_file.write(join_paragraph(paragraph_lines) + '\\n')\n",
        "                        inside_paragraph = False\n",
        "                        paragraph_lines = []\n",
        "                    else:\n",
        "                        inside_paragraph = True\n",
        "                        paragraph_lines.append(line.strip())\n",
        "\n",
        "                # Ensure the last paragraph is processed\n",
        "                if inside_paragraph and len(paragraph_lines) < 9:\n",
        "                    output_file.write(join_paragraph(paragraph_lines) + '\\n')\n",
        "\n",
        "def join_paragraph(paragraph_lines):\n",
        "    return ' '.join(paragraph_lines)\n",
        "\n",
        "# Specify the path to the main folder containing subfolders and text files\n",
        "main_folder = 'filepath'  # Replace with the actual main folder path\n",
        "\n",
        "# Specify the path to the output folder\n",
        "output_folder = 'FINAL'  # Replace with the desired output folder path\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Start processing the main folder and its subfolders\n",
        "process_folder(main_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "-dcl-QV0R93r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the path to the main folder containing subfolders and text files\n",
        "main_folder = 'FINAL_file'  # Replace with the actual main folder path\n",
        "\n",
        "# Create an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Initialize variables to track the last folder\n",
        "last_folder_name = None\n",
        "last_df = None\n",
        "\n",
        "# Define a function to extract the page number from a filename\n",
        "def extract_page_number(filename):\n",
        "    page_prefix = \"page\"\n",
        "    if page_prefix in filename:\n",
        "        start = filename.index(page_prefix) + len(page_prefix)\n",
        "        end = filename.index('.', start)\n",
        "        return int(filename[start:end])\n",
        "    return 0  # Return 0 for filenames without a page number\n",
        "\n",
        "# Iterate through subfolders and files\n",
        "for folder, subfolders, files in os.walk(main_folder):\n",
        "    for filename in files:\n",
        "        if filename.endswith('.txt'):\n",
        "            folder_name = os.path.basename(folder)\n",
        "\n",
        "            # Sort the files in each folder based on page numbers\n",
        "            file_list = sorted(files, key=extract_page_number)\n",
        "\n",
        "            # Create an empty list to store DataFrames for this folder\n",
        "            folder_dfs = []\n",
        "\n",
        "            for filename in file_list:\n",
        "                file_path = os.path.join(folder, filename)\n",
        "                with open(file_path, 'r') as file:\n",
        "                    content = file.readlines()\n",
        "\n",
        "                    # Remove trailing empty lines and strip each line\n",
        "                    cleaned_content = [line.strip() for line in content if line.strip()]\n",
        "\n",
        "                    # Extract the page number (left side of the filename before the period)\n",
        "                    page_number = extract_page_number(filename)\n",
        "\n",
        "                    # Create a DataFrame for each file with the page number as an integer\n",
        "                    df = pd.DataFrame({'Reports': [folder_name] * len(cleaned_content), 'Page Numbers': [page_number] * len(cleaned_content), 'Content': cleaned_content})\n",
        "                    folder_dfs.append(df)\n",
        "\n",
        "            # Concatenate DataFrames for this folder\n",
        "            if folder_dfs:\n",
        "                folder_df = pd.concat(folder_dfs, ignore_index=True)\n",
        "\n",
        "                # Check if this folder is different from the last processed folder\n",
        "                if folder_name != last_folder_name:\n",
        "                    dfs.append(folder_df)\n",
        "                    last_folder_name = folder_name\n",
        "                else:\n",
        "                    # Compare with the last DataFrame to prevent repetition\n",
        "                    if last_df is None or not last_df.equals(folder_df):\n",
        "                        dfs.append(folder_df)\n",
        "\n",
        "                last_df = folder_df\n",
        "\n",
        "# Concatenate all DataFrames into one\n",
        "result_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Convert 'Page Numbers' column to integers\n",
        "result_df['Page Numbers'] = result_df['Page Numbers'].astype(int)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "id": "QTBrrKetU-Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to excel\n",
        "with pd.ExcelWriter('file.xlsx') as writer:\n",
        "    result_df.to_excel(writer, sheet_name='Files Details')"
      ],
      "metadata": {
        "id": "8cXxUSllVzc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NOTE Extraction**"
      ],
      "metadata": {
        "id": "kmjKgN9zhKR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def process_folder(input_folder, output_folder):\n",
        "    for item in os.listdir(input_folder):\n",
        "        item_path = os.path.join(input_folder, item)\n",
        "\n",
        "        if os.path.isdir(item_path):\n",
        "            # If it's a subfolder, recursively process it\n",
        "            subfolder_name = os.path.basename(item_path)\n",
        "            subfolder_output = os.path.join(output_folder, subfolder_name)\n",
        "            os.makedirs(subfolder_output, exist_ok=True)\n",
        "            process_folder(item_path, subfolder_output)\n",
        "        elif item.endswith('.txt'):\n",
        "            # If it's a file, process it and save the output\n",
        "            input_file_path = item_path\n",
        "\n",
        "            # Define the output file path in the output folder\n",
        "            output_file_path = os.path.join(output_folder, item)\n",
        "\n",
        "            # Call the functions to process the file and save the output\n",
        "            check_lines(input_file_path, output_file_path)\n",
        "            check_lines_and_words(input_file_path, output_file_path)\n",
        "\n",
        "def check_lines(input_file_path, output_file_path):\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "        total_lines = len(lines)\n",
        "        i = 0\n",
        "        current_line = \"\"\n",
        "\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            while i < total_lines:\n",
        "                words = lines[i].strip().split()\n",
        "\n",
        "                if len(words) >= 2 and words[0].lower() == 'note' and words[1][0].isdigit():\n",
        "                    current_line = lines[i].strip()\n",
        "\n",
        "                    # Check if there is a third word\n",
        "                    if len(words) >= 3:\n",
        "                        output_file.write(current_line + '\\n')\n",
        "                    else:\n",
        "                        # Join the current line with the next line since there's no third word\n",
        "                        current_line += \" \" + lines[i + 2].strip()\n",
        "                        output_file.write(current_line + '\\n')\n",
        "\n",
        "                    i += 1  # Move to the next line\n",
        "\n",
        "                i += 1\n",
        "\n",
        "def check_lines_and_words(input_file_path, output_file_path):\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        for line_number, line in enumerate(input_file, start=1):\n",
        "            # Split the line into words\n",
        "            words = line.strip().split()\n",
        "\n",
        "            if len(words) >= 2 and len(words) < 10:  # Check if there are at least 2 words and less than 10 words\n",
        "                # Check if the first word starts with a digit and ends with a period\n",
        "                # or ends with a closing parenthesis ')' and the second word starts with an uppercase letter\n",
        "                first_word = words[0]\n",
        "                second_word = words[1]\n",
        "\n",
        "                if (first_word[0].isdigit() and (first_word.endswith('.') or first_word.endswith(')')) and\n",
        "                        second_word[0].isupper()):\n",
        "                    with open(output_file_path, 'a') as output_file:\n",
        "                        output_file.write(f\"{line.strip()}\\n\")\n",
        "\n",
        "# Specify the path to the main folder containing subfolders and text files\n",
        "main_folder = 'extracted files'  # Replace with the actual main folder path\n",
        "\n",
        "# Specify the path to the output folder\n",
        "output_folder = 'output_filenmae'  # Replace with the desired output folder path\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Start processing the main folder and its subfolders\n",
        "process_folder(main_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "Zvxgqeb8hSdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outputing the Notes in a Dataframe\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the path to the main folder containing subfolders and text files\n",
        "main_folder = 'filepath'  # Replace with the actual main folder path\n",
        "\n",
        "# Create an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Initialize variables to track the last folder\n",
        "last_folder_name = None\n",
        "last_df = None\n",
        "\n",
        "# Define a function to extract the page number from a filename\n",
        "def extract_page_number(filename):\n",
        "    page_prefix = \"page\"\n",
        "    if page_prefix in filename:\n",
        "        start = filename.index(page_prefix) + len(page_prefix)\n",
        "        end = filename.index('.', start)\n",
        "        return int(filename[start:end])\n",
        "    return 0  # Return 0 for filenames without a page number\n",
        "\n",
        "# Iterate through subfolders and files\n",
        "for folder, subfolders, files in os.walk(main_folder):\n",
        "    for filename in files:\n",
        "        if filename.endswith('.txt'):\n",
        "            folder_name = os.path.basename(folder)\n",
        "\n",
        "            # Sort the files in each folder based on page numbers\n",
        "            file_list = sorted(files, key=extract_page_number)\n",
        "\n",
        "            # Create an empty list to store DataFrames for this folder\n",
        "            folder_dfs = []\n",
        "\n",
        "            for filename in file_list:\n",
        "                file_path = os.path.join(folder, filename)\n",
        "                with open(file_path, 'r') as file:\n",
        "                    content = file.readlines()\n",
        "\n",
        "                    # Remove trailing empty lines and strip each line\n",
        "                    cleaned_content = [line.strip() for line in content if line.strip()]\n",
        "\n",
        "                    # Extract the page number (left side of the filename before the period)\n",
        "                    page_number = extract_page_number(filename)\n",
        "\n",
        "                    # Create a DataFrame for each file with the page number as an integer\n",
        "                    df = pd.DataFrame({'Reports': [folder_name] * len(cleaned_content), 'Page Numbers': [page_number] * len(cleaned_content), 'Content': cleaned_content})\n",
        "                    folder_dfs.append(df)\n",
        "\n",
        "            # Concatenate DataFrames for this folder\n",
        "            if folder_dfs:\n",
        "                folder_df = pd.concat(folder_dfs, ignore_index=True)\n",
        "\n",
        "                # Check if this folder is different from the last processed folder\n",
        "                if folder_name != last_folder_name:\n",
        "                    dfs.append(folder_df)\n",
        "                    last_folder_name = folder_name\n",
        "                else:\n",
        "                    # Compare with the last DataFrame to prevent repetition\n",
        "                    if last_df is None or not last_df.equals(folder_df):\n",
        "                        dfs.append(folder_df)\n",
        "\n",
        "                last_df = folder_df\n",
        "\n",
        "# Concatenate all DataFrames into one\n",
        "result_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Convert 'Page Numbers' column to integers\n",
        "result_df['Page Numbers'] = result_df['Page Numbers'].astype(int)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "id": "CDC3mugYmUzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to excel\n",
        "with pd.ExcelWriter('note.xlsx') as writer:\n",
        "    result_df.to_excel(writer, sheet_name='Files Details')"
      ],
      "metadata": {
        "id": "VKI_jz8Emhj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Table, Image extraction**"
      ],
      "metadata": {
        "id": "wFFSBWgvs-jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qqq PyMuPDF"
      ],
      "metadata": {
        "id": "Dud5CXdrtILY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qqq pillow"
      ],
      "metadata": {
        "id": "3smWM5oJtnEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q tabula"
      ],
      "metadata": {
        "id": "hHtJuQDWtmKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tabula-py pandas"
      ],
      "metadata": {
        "id": "3WNT9c55tpqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import fitz\n",
        "import io\n",
        "from PIL import Image, ImageDraw"
      ],
      "metadata": {
        "id": "WeKqND2utzm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image, ImageDraw\n",
        "import io\n",
        "from tabula import read_pdf\n",
        "\n",
        "def extract_tables_and_images(pdf_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Open the PDF file\n",
        "    with fitz.open(pdf_path) as pdf_doc:\n",
        "        for page_num in range(pdf_doc.page_count):\n",
        "            # Extract tables using tabula\n",
        "            tables = read_pdf(pdf_path, pages=page_num + 1)\n",
        "\n",
        "            # Save each table as an Excel file with page number\n",
        "            for table_num, table in enumerate(tables, start=1):\n",
        "                if table is not None and not table.empty:\n",
        "                    table.to_excel(os.path.join(output_folder, f\"table_tabula_page_{page_num + 1}_table_{table_num}.xlsx\"), index=False)\n",
        "\n",
        "            # Extract images\n",
        "            page = pdf_doc[page_num]\n",
        "            image_list = page.get_images()\n",
        "\n",
        "\n",
        "            # Display and save images\n",
        "            for image_index, img in enumerate(image_list, start=1):\n",
        "                xref = img[0]\n",
        "                image_bytes = pdf_doc.extract_image(xref)[\"image\"]\n",
        "                pil_image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "                # Display image\n",
        "                pil_image.show()\n",
        "\n",
        "                # Save image\n",
        "                image_path = os.path.join(output_folder, f\"image_page_{page_num + 1}_image_{image_index}.png\")\n",
        "                pil_image.save(image_path)\n",
        "\n",
        "                print(f\"[+] Image {image_index} on page {page_num + 1} saved as {image_path}\")\n",
        "\n",
        "# Example usage\n",
        "pdf_path = 'filepath.pdf'\n",
        "output_folder = 'OUTPUT_FOLDER'\n",
        "extract_tables_and_images(pdf_path, output_folder)"
      ],
      "metadata": {
        "id": "v8FOo3m5t0fU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}